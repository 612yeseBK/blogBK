---
title: jieba之分词
categories: NLP
tags:
  - 分词
  - jieba
date: 2018-11-09 16:42:01
---
<Excerpt in index | 首页摘要>摘要：这是一篇介绍如何使用jieba去分词的博客<!-- more -->
<The rest of contents | 余下全文>

# 导入

```
import jieba
```

# 建立短句

```
s = u'我想和女朋友一起去北京故宫博物院参观和闲逛。'
```

这里的u是告诉python是使用Unicode编码解读里面的数据，一般在文件开头加上# -*- coding: UTF-8 -*-也可以做到

# 精确模式

```
cut = jieba.cut(s)
print("【精确模式下输出】")
print(cut) 
# <generator object Tokenizer.cut at 0x0000023490DA3888>
# 这里输出的是一个generator
print(','.join(cut))  
# ','.join(cut)表示使用","将cut里面的序列连接起来
# 我,想,和,女朋友,一起,去,北京故宫博物院,参观,和,闲逛,。
```

# 全局模式

```
cutall=jieba.cut(s,cut_all = True)
print("【全局模式下输出】")
print(','.join(cutall))
# 我,想,和,女朋友,朋友,一起,去,北京,北京故宫,北京故宫博物院,故宫,故宫博物院,博物,博物院,参观,和,闲逛,,
```

全局模式下会尽可能多的对字符串进行切分

# 搜索引擎模式

```
cutsearch = jieba.cut_for_search(s)
print('【搜索引擎模式下输出】')
print(','.join(cutsearch))
# 我,想,和,朋友,女朋友,一起,去,北京,故宫,博物,博物院,北京故宫博物院,参观,和,闲逛,。
```
在精确模式的基础上，对长词在此划分

# 使用lcut获取list

```
cut_s = jieba.lcut(s)
print(type(cut_s)) 
# <class 'list'> 这里返回的是list
print(cut_s)
# ['我', '想', '和', '女朋友', '一起', '去', '北京故宫博物院', '参观', '和', '闲逛', '。']

lcut_for_search = jieba.lcut_for_search(s) # 这里也是lcut
print(lcut_for_search)  # 返回list
# ['我', '想', '和', '朋友', '女朋友', '一起', '去', '北京', '故宫', '博物', '博物院', '北京故宫博物院', '参观', '和', '闲逛', '。']
```

# 综合上述代码

```
# -*- coding: UTF-8 -*-
import numpy as np
# import tensorflow as tf

"""
Created by gaoyw on 2018/11/9
"""
import jieba
s = '我想和女朋友一起去北京故宫博物院参观和闲逛。'
cut = jieba.cut(s)

print('【精确模式下输出】')
print(cut)
print(','.join(cut))

cutall=jieba.cut(s,cut_all = True)
print("【全局模式下输出】")
print(','.join(cutall))

cutsearch = jieba.cut_for_search(s)
print('【搜索引擎模式下输出】')
print(','.join(cutsearch))

cut_s = jieba.lcut(s)
print(type(cut_s))
print(cut_s)

lcut_for_search = jieba.lcut_for_search(s)
print(lcut_for_search)
```